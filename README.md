In this repository are some examples of Spark processing in Scala, Python, and SQL.

Prefer to use Spark SQL and the Dataset API where suitable to take advantage of Spark SQL's optimized execution engine.

RDDs are more performant when the operation is not SQL-like.






Standardizing different weights - whether kilograms, grams, ounces 
https://towardsdatascience.com/interactively-analyse-100gb-of-json-data-with-spark-e018f9436e76




### archive 

understand the sample datasets documentation - not much
think of different ways to slice and dice the data 
write sample notebooks here, then run the whole notebook in spark
import notebook into databricks?


basic overview of spark on azure databricks
https://tomaztsql.wordpress.com/2020/12/10/advent-of-2020-day-10-using-azure-databricks-notebooks-with-sql-for-data-engineering-tasks/
seems like mostly will be using sql

speech to text - https://www.johnsnowlabs.com/converting-speech-to-text-with-spark-nlp-and-python/  
how about calling apis?

datasets - typed - compile checks 

https://louisazhou.gitbook.io/notes/spark/data-cleaning-with-apache-spark

common tasks in iot pipeline
https://futureiot.tech/building-an-iot-data-pipeline/
